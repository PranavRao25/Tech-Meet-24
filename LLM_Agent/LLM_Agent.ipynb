{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NLMfTvpg8nh"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ResponseSchema(BaseModel):\n",
    "    \"\"\"Schema for structured output\"\"\"\n",
    "    answer: str = Field(description=\"The direct answer to the question\")\n",
    "    reasoning: str = Field(description=\"Explanation of how the answer was derived from the context\")\n",
    "    sources: List[str] = Field(description=\"List of sources used from the context\")\n",
    "\n",
    "class GeminiOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parser to structure the LLM output\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> ResponseSchema:\n",
    "        \"\"\"Parse the LLM output into structured format\"\"\"\n",
    "        try:\n",
    "            # Split the response into sections\n",
    "            sections = text.split(\"\\n\\n\")\n",
    "\n",
    "            answer = sections[0].replace(\"Answer: \", \"\").strip()\n",
    "            reasoning = sections[1].replace(\"Reasoning: \", \"\").strip()\n",
    "            sources = sections[2].replace(\"Sources: \", \"\").strip().split(\", \")\n",
    "\n",
    "            return ResponseSchema(\n",
    "                answer=answer,\n",
    "                reasoning=reasoning,\n",
    "                sources=sources\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse LLM output: {e}\")\n",
    "\n",
    "class LLMAgent:\n",
    "    \"\"\"LLM Agent class for RAG pipeline integration\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        google_api_key: str,\n",
    "        model_name: str = \"gemini-pro\",\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1000\n",
    "    ):\n",
    "        \"\"\"Initialize the LLM Agent\"\"\"\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            google_api_key=google_api_key,\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_tokens\n",
    "        )\n",
    "        self.output_parser = GeminiOutputParser()\n",
    "\n",
    "        # Define the prompt template\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"\n",
    "            You are a helpful AI assistant. Using the provided context, answer the question.\n",
    "            Format your response in the following way:\n",
    "\n",
    "            Answer: [Provide a clear, direct answer]\n",
    "\n",
    "            Reasoning: [Explain how you arrived at the answer using the context]\n",
    "\n",
    "            Sources: [List the relevant sources from the context]\n",
    "\n",
    "            The inputs are\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Create LangChain chain\n",
    "        self.chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.prompt_template\n",
    "        )\n",
    "\n",
    "    def process_query(\n",
    "        self,\n",
    "        question: str,\n",
    "        context: str\n",
    "    ) -> ResponseSchema:\n",
    "        \"\"\"\n",
    "        Process a query using the provided context\n",
    "\n",
    "        Args:\n",
    "            question: User's question\n",
    "            context: List of context documents with their metadata\n",
    "\n",
    "        Returns:\n",
    "            Structured response containing answer, reasoning, and sources\n",
    "        \"\"\"\n",
    "        # Format context for the prompt\n",
    "\n",
    "        # formatted_context = \"\\n\".join([\n",
    "        #     f\"Document {i+1}: {doc['content']} (Source: {doc.get('source', 'Unknown')})\"\n",
    "        #     for i, doc in enumerate(context)\n",
    "        # ])\n",
    "\n",
    "        # Get response from LLM\n",
    "        response = self.chain.run(\n",
    "            context=context,\n",
    "            question=question\n",
    "        )\n",
    "\n",
    "        # Parse and return structured output\n",
    "        return self.output_parser.parse(response)\n",
    "\n",
    "    def get_sources(self, response: ResponseSchema) -> List[str]:\n",
    "        \"\"\"Extract sources from the response\"\"\"\n",
    "        return response.sources\n",
    "\n",
    "    def get_answer(self, response: ResponseSchema) -> str:\n",
    "        \"\"\"Extract the main answer from the response\"\"\"\n",
    "        return response.answer\n",
    "\n",
    "    def get_reasoning(self, response: ResponseSchema) -> str:\n",
    "        \"\"\"Extract the reasoning from the response\"\"\"\n",
    "        return response.reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9_25ufvnH-R"
   },
   "source": [
    "Testing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMFoluwBhijf",
    "outputId": "c70941ab-9283-4651-cafc-5a93ebc787c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='Paris is the capital of France.' reasoning='The context states that \"Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower.\"' sources=['The context provided.']\n"
     ]
    }
   ],
   "source": [
    "llm=LLMAgent(google_api_key=\"Your API key\",model_name=\"gemini-pro\")\n",
    "context_france=\"\\\n",
    "France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower. The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and the vast Palace of Versailles attest to its rich history.\\\n",
    "\"\n",
    "output=llm.process_query(question=\"What is the capital of France\",context=context_france)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuFw1NAskSmJ",
    "outputId": "e85270b4-a3bb-45af-880d-0cdc2d056029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer =  Paris is the capital of France.\n",
      "reasoning =  The context states that \"Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower.\"\n",
      "Sources =  ['The context provided.']\n"
     ]
    }
   ],
   "source": [
    "print('Answer = ',output.answer)\n",
    "print('reasoning = ',output.reasoning)\n",
    "print('Sources = ',output.sources)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
